{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QPafvdblVt2U"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "9BZeR6uAcmMR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
        "import numpy as np\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "import sys\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import col\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Jena_Climate\").getOrCreate()"
      ],
      "metadata": {
        "id": "yrr2-VS5cuMQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "def load_data():\n",
        "    df=spark.read.option(\"header\", \"true\").csv(\"jena_climate_2009_2016.csv\")\n",
        "    return df\n",
        "\n",
        "#Use MinMaxScaler to get data between 0 and 1\n",
        "def preprocess_data(df):\n",
        "    #consider temperature as feature to predict weather\n",
        "    df_ftrs=df.select(['T (degC)'])\n",
        "    #normalize data\n",
        "    vector_asblr = VectorAssembler(inputCols=df_ftrs.columns, outputCol=\"x\")\n",
        "    norm_scalar = MinMaxScaler(inputCol=\"x\", outputCol=\"y\")\n",
        "    pipe_line = Pipeline(stages=[vector_asblr, norm_scalar])\n",
        "    scale_model = pipe_line.fit(df_ftrs)\n",
        "    scale_features = scale_model.transform(df_ftrs)\n",
        "\n",
        "    #separate feature columns\n",
        "    scale_features = scale_features.select(create_lst_from_arr(F.col(\"y\")).alias(\"split_ftrs\")).select([F.col(\"split_ftrs\")[f].alias(\"T_Scaled\") for f in range(len(df_ftrs.columns))])\n",
        "\n",
        "    return scale_features\n",
        "\n",
        "# split array\n",
        "def create_lst_from_arr(colum):\n",
        "    def create_lst(vec):\n",
        "        return vec.toArray().tolist()\n",
        "    return F.udf(create_lst, ArrayType(DoubleType()))(colum)\n",
        "\n",
        "#Preparing dataset based on time sequence\n",
        "def prepare_data(wt_data, time_sequence = 1):\n",
        "    i=0\n",
        "    return_data = []\n",
        "    wt_data = [z[0] for z in wt_data.select('T_Scaled').toLocalIterator()]\n",
        "    while i < len(wt_data) - time_sequence - 1:\n",
        "       return_data.append((wt_data[i:(i + time_sequence)], wt_data[i + time_sequence]))\n",
        "       i += 1\n",
        "    return return_data\n",
        "\n",
        "#Split 80 % train and 20 % test\n",
        "def train_test_split(wt_data,  time_sequence):\n",
        "    total_smpls = wt_data.count()\n",
        "    train_data_size=int(total_smpls*0.80)\n",
        "\n",
        "    traind = wt_data.limit(train_data_size)\n",
        "    testd  = wt_data.subtract(traind)\n",
        "\n",
        "    traind = spark.createDataFrame(prepare_data(traind), ['X','Y'], time_sequence)\n",
        "    testd = spark.createDataFrame(prepare_data(testd), ['X','Y'],  )\n",
        "\n",
        "    return traind, testd"
      ],
      "metadata": {
        "id": "-hGzUsFNtVxx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform forward propagation\n",
        "def frwd_prop(inp, inp_w, outp_w, hid_w):\n",
        "    i=0\n",
        "    # out_p=[]\n",
        "    state = []\n",
        "    state.append(np.zeros((hd_lyr, 1)))\n",
        "    while i < inp.shape[0]:\n",
        "        z = (inp_w @ inp[[i]].T) + (hid_w @ state[-1])\n",
        "        o = activefunc(z)\n",
        "        state.append(o)\n",
        "        i += 1\n",
        "    out_p =  outp_w @ state[-1]\n",
        "    return (state,out_p)\n",
        "\n",
        "# activation function - tanh\n",
        "def activefunc(inpt, differen= False):\n",
        "    if(differen== True):\n",
        "        return (1 - np.square(np.tanh(inpt)))\n",
        "    return np.tanh(inpt)\n",
        "\n",
        "def backpropogation(inputs, targets, hidden_states, out_ps, wt_ih,  wt_ho,wt_hh):\n",
        "    loss = np.mean(np.square(inputs - out_ps))\n",
        "    # out_p layer gradients\n",
        "    out_err = out_ps - targets\n",
        "    gradient_out_p = out_err.dot(hidden_states[-1].T)\n",
        "\n",
        "    # Initialize gradients\n",
        "    grd_hh = np.zeros_like(wt_hh)\n",
        "    grd_ih = np.zeros_like(wt_ih)\n",
        "    grd_ho = np.zeros_like(wt_ho)\n",
        "    next_error = np.zeros((wt_hh.shape[0], 1))\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        error = np.dot(wt_ho.T, out_err) + np.dot(wt_hh.T, next_error)\n",
        "        gradient = (1 - hidden_states[t] ** 2) * error\n",
        "        grd_ho += out_err.dot(hidden_states[t].T)\n",
        "        grd_hh += np.dot(gradient, hidden_states[t - 1].T)\n",
        "        grd_ih += np.dot(gradient, inputs[t].reshape(-1, 1))\n",
        "        next_error = np.dot(wt_hh.T, gradient)\n",
        "\n",
        "    # Update weights using gradients and learning rate\n",
        "    wt_ho -= learn_rate * grd_ho\n",
        "    wt_hh -= learn_rate * grd_hh\n",
        "    wt_ih -= learn_rate * grd_ih\n",
        "\n",
        "    return wt_ih, wt_hh, wt_ho\n",
        "\n",
        "#model training for # of epochs\n",
        "def train_model(train,epochs):\n",
        "    #initialize w\n",
        "    inp_w,outp_w,hid_w = (np.random.uniform(0, 1, (hd_lyr, i_lyr)) / 2),(np.random.uniform(0, 1, (o_lyr, hd_lyr)) / 2),(np.random.uniform(0, 1, (hd_lyr, hd_lyr)) / 2)\n",
        "\n",
        "    for iter in range(epochs):\n",
        "        if(iter == epochs-1):\n",
        "            trn_prd = []\n",
        "        for i in train.rdd.toLocalIterator():\n",
        "            xd = np.array(i['X'])\n",
        "            xd = xd.reshape((1,xd.shape[0]))\n",
        "            state, out_p = frwd_prop(xd,  inp_w, outp_w, hid_w)\n",
        "            if(iter == epochs-1):\n",
        "                trn_prd.append(out_p.tolist()[0])\n",
        "            inp_w, hid_w, outp_w = backpropogation(xd, i['Y'], state,out_p, inp_w, outp_w, hid_w)\n",
        "    trn_prd = np.array(trn_prd).T[0]\n",
        "    return trn_prd, inp_w, hid_w, outp_w\n",
        "\n",
        "#Based on weights trained, test model\n",
        "def test_model(testd, inp_w, hid_w, outp_w):\n",
        "    tst_prd = []\n",
        "    for i in testd.rdd.toLocalIterator():\n",
        "        xd = np.array(i['X'])\n",
        "        xd = xd.reshape((xd.shape[0],1))\n",
        "        state, out_p = frwd_prop(xd, inp_w, hid_w, outp_w)\n",
        "        tst_prd.append(out_p.tolist()[0])\n",
        "    tst_prd = np.array(tst_prd).T[0]\n",
        "    return tst_prd"
      ],
      "metadata": {
        "id": "P5F2s0-yR6eZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #load and preprocess data\n",
        "    data = load_data()\n",
        "\n",
        "    print(\"-----Weather Data-----\")\n",
        "    data.show()\n",
        "    print(\"-----Weather Data Schema-----\")\n",
        "    data.printSchema()\n",
        "\n",
        "    data = data.withColumn(\"p (mbar)\", when(col(\"p (mbar)\") == \"\", None).otherwise(col(\"p (mbar)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"T (degC)\", when(col(\"T (degC)\") == \"\", None).otherwise(col(\"T (degC)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"VPmax (mbar)\", when(col(\"VPmax (mbar)\") == \"\", None).otherwise(col(\"VPmax (mbar)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"VPdef (mbar)\", when(col(\"VPdef (mbar)\") == \"\", None).otherwise(col(\"VPdef (mbar)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"sh (g/kg)\", when(col(\"sh (g/kg)\") == \"\", None).otherwise(col(\"sh (g/kg)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"rho (g/m**3)\", when(col(\"rho (g/m**3)\") == \"\", None).otherwise(col(\"rho (g/m**3)\").cast(\"double\")))\n",
        "    data = data.withColumn(\"wv (m/s)\", when(col(\"wv (m/s)\") == \"\", None).otherwise(col(\"wv (m/s)\").cast(\"double\")))\n",
        "\n",
        "    # Check the schema again and proceed with further analysis\n",
        "    print(\"-----Processed Weather Data Schema-----\")\n",
        "    data.printSchema()\n",
        "    preprocessed_data = preprocess_data(data)\n",
        "\n",
        "    print(\"----Processed Data-----\")\n",
        "    preprocessed_data.show()\n",
        "\n",
        "    #set layers size\n",
        "    i_lyr,hd_lyr,o_lyr = 1,20,1\n",
        "\n",
        "    #setting hyperparameters\n",
        "    learn_rate = 0.01\n",
        "    time_sequence = [1,5,10]\n",
        "    epochs = [25,50]\n",
        "\n",
        "    for steps in time_sequence:\n",
        "        for iter in epochs:\n",
        "\n",
        "            print(\"----***----RNN training with time_sequence : {} and epochs : {}\".format(steps, iter))\n",
        "            #perform split data\n",
        "            train, test = train_test_split(preprocessed_data, steps)\n",
        "\n",
        "            #Model- rnn train\n",
        "            train_prd, input_w, hidden_w, output_w = train_model(train, iter)\n",
        "\n",
        "            #Model- rnn test\n",
        "            test_prd = test_model(test, input_w, hidden_w, output_w)\n",
        "\n",
        "            #To evaluate, convert into proper format\n",
        "            prd_trn = train_prd.tolist()\n",
        "            prd_tst = test_prd.tolist()\n",
        "\n",
        "            actual_train = train.rdd.map(lambda x: x[1]).collect()\n",
        "            actual_test = test.rdd.map(lambda x: x[1]).collect()\n",
        "\n",
        "            train = spark.createDataFrame(zip(actual_train, prd_trn), ['actual_output', 'predicted_output'])\n",
        "            test = spark.createDataFrame(zip(actual_test, prd_tst), ['actual_output', 'predicted_output'])\n",
        "\n",
        "            train_values = train.rdd.map(tuple)\n",
        "            test_values = test.rdd.map(tuple)\n",
        "\n",
        "\n",
        "            #Performance evaluation of the model\n",
        "            print(\"\\n\\n----Training Evaluation----\")\n",
        "            performance = RegressionMetrics(train_values)\n",
        "\n",
        "            print(\"MSE: {}\".format(performance.meanSquaredError))\n",
        "            print(\"RMSE: {}\".format(performance.rootMeanSquaredError))\n",
        "            print(\"MAE: {}\".format(performance.meanAbsoluteError))\n",
        "\n",
        "\n",
        "            print(\"\\n\\n----Testing Evaluation----\")\n",
        "            performance = RegressionMetrics(test_values)\n",
        "\n",
        "            print(\"MSE: {}\".format(performance.meanSquaredError))\n",
        "            print(\"RMSE: {}\".format(performance.rootMeanSquaredError))\n",
        "            print(\"MAE: {}\\n\\n\".format(performance.meanAbsoluteError))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz5gGAgEc5Lh",
        "outputId": "1731b5b9-f100-4f9b-fba1-0c03d12826b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Weather Data-----\n",
            "+-------------------+--------+--------+--------+-----------+------+------------+------------+------------+---------+---------------+------------+--------+-------------+--------+\n",
            "|          Date Time|p (mbar)|T (degC)|Tpot (K)|Tdew (degC)|rh (%)|VPmax (mbar)|VPact (mbar)|VPdef (mbar)|sh (g/kg)|H2OC (mmol/mol)|rho (g/m**3)|wv (m/s)|max. wv (m/s)|wd (deg)|\n",
            "+-------------------+--------+--------+--------+-----------+------+------------+------------+------------+---------+---------------+------------+--------+-------------+--------+\n",
            "|01.01.2009 00:10:00|  996.52|   -8.02|  265.40|      -8.90| 93.30|        3.33|        3.11|        0.22|     1.94|           3.12|     1307.75|    1.03|         1.75|  152.30|\n",
            "|01.01.2009 00:20:00|  996.57|   -8.41|  265.01|      -9.28| 93.40|        3.23|        3.02|        0.21|     1.89|           3.03|     1309.80|    0.72|         1.50|  136.10|\n",
            "|01.01.2009 00:30:00|  996.53|   -8.51|  264.91|      -9.31| 93.90|        3.21|        3.01|        0.20|     1.88|           3.02|     1310.24|    0.19|         0.63|  171.60|\n",
            "|01.01.2009 00:40:00|  996.51|   -8.31|  265.12|      -9.07| 94.20|        3.26|        3.07|        0.19|     1.92|           3.08|     1309.19|    0.34|         0.50|  198.00|\n",
            "|01.01.2009 00:50:00|  996.51|   -8.27|  265.15|      -9.04| 94.10|        3.27|        3.08|        0.19|     1.92|           3.09|     1309.00|    0.32|         0.63|  214.30|\n",
            "|01.01.2009 01:00:00|  996.50|   -8.05|  265.38|      -8.78| 94.40|        3.33|        3.14|        0.19|     1.96|           3.15|     1307.86|    0.21|         0.63|  192.70|\n",
            "|01.01.2009 01:10:00|  996.50|   -7.62|  265.81|      -8.30| 94.80|        3.44|        3.26|        0.18|     2.04|           3.27|     1305.68|    0.18|         0.63|  166.50|\n",
            "|01.01.2009 01:20:00|  996.50|   -7.62|  265.81|      -8.36| 94.40|        3.44|        3.25|        0.19|     2.03|           3.26|     1305.69|    0.19|         0.50|  118.60|\n",
            "|01.01.2009 01:30:00|  996.50|   -7.91|  265.52|      -8.73| 93.80|        3.36|        3.15|        0.21|     1.97|           3.16|     1307.17|    0.28|         0.75|  188.50|\n",
            "|01.01.2009 01:40:00|  996.53|   -8.43|  264.99|      -9.34| 93.10|        3.23|        3.00|        0.22|     1.88|           3.02|     1309.85|    0.59|         0.88|  185.00|\n",
            "|01.01.2009 01:50:00|  996.62|   -8.76|  264.66|      -9.66| 93.10|        3.14|        2.93|        0.22|     1.83|           2.94|     1311.64|    0.45|         0.88|  183.20|\n",
            "|01.01.2009 02:00:00|  996.62|   -8.88|  264.54|      -9.77| 93.20|        3.12|        2.90|        0.21|     1.81|           2.91|     1312.25|    0.25|         0.63|  190.30|\n",
            "|01.01.2009 02:10:00|  996.63|   -8.85|  264.57|      -9.70| 93.50|        3.12|        2.92|        0.20|     1.82|           2.93|     1312.11|    0.16|         0.50|  158.30|\n",
            "|01.01.2009 02:20:00|  996.74|   -8.83|  264.58|      -9.68| 93.50|        3.13|        2.92|        0.20|     1.83|           2.93|     1312.15|    0.36|         0.63|  184.80|\n",
            "|01.01.2009 02:30:00|  996.81|   -8.66|  264.74|      -9.46| 93.90|        3.17|        2.98|        0.19|     1.86|           2.99|     1311.37|    0.33|         0.75|  155.90|\n",
            "|01.01.2009 02:40:00|  996.81|   -8.66|  264.74|      -9.50| 93.60|        3.17|        2.97|        0.20|     1.85|           2.98|     1311.38|    0.07|         0.50|  272.40|\n",
            "|01.01.2009 02:50:00|  996.86|   -8.70|  264.70|      -9.55| 93.50|        3.16|        2.95|        0.21|     1.85|           2.96|     1311.64|    0.32|         0.63|  219.20|\n",
            "|01.01.2009 03:00:00|  996.84|   -8.81|  264.59|      -9.66| 93.50|        3.13|        2.93|        0.20|     1.83|           2.94|     1312.18|    0.18|         0.63|  167.20|\n",
            "|01.01.2009 03:10:00|  996.87|   -8.84|  264.56|      -9.69| 93.50|        3.13|        2.92|        0.20|     1.83|           2.93|     1312.37|    0.07|         0.25|  129.30|\n",
            "|01.01.2009 03:20:00|  996.97|   -8.94|  264.45|      -9.82| 93.30|        3.10|        2.89|        0.21|     1.81|           2.90|     1313.01|    0.10|         0.63|  115.30|\n",
            "+-------------------+--------+--------+--------+-----------+------+------------+------------+------------+---------+---------------+------------+--------+-------------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "-----Weather Data Schema-----\n",
            "root\n",
            " |-- Date Time: string (nullable = true)\n",
            " |-- p (mbar): string (nullable = true)\n",
            " |-- T (degC): string (nullable = true)\n",
            " |-- Tpot (K): string (nullable = true)\n",
            " |-- Tdew (degC): string (nullable = true)\n",
            " |-- rh (%): string (nullable = true)\n",
            " |-- VPmax (mbar): string (nullable = true)\n",
            " |-- VPact (mbar): string (nullable = true)\n",
            " |-- VPdef (mbar): string (nullable = true)\n",
            " |-- sh (g/kg): string (nullable = true)\n",
            " |-- H2OC (mmol/mol): string (nullable = true)\n",
            " |-- rho (g/m**3): string (nullable = true)\n",
            " |-- wv (m/s): string (nullable = true)\n",
            " |-- max. wv (m/s): string (nullable = true)\n",
            " |-- wd (deg): string (nullable = true)\n",
            "\n",
            "-----Processed Weather Data Schema-----\n",
            "root\n",
            " |-- Date Time: string (nullable = true)\n",
            " |-- p (mbar): double (nullable = true)\n",
            " |-- T (degC): double (nullable = true)\n",
            " |-- Tpot (K): string (nullable = true)\n",
            " |-- Tdew (degC): string (nullable = true)\n",
            " |-- rh (%): string (nullable = true)\n",
            " |-- VPmax (mbar): double (nullable = true)\n",
            " |-- VPact (mbar): string (nullable = true)\n",
            " |-- VPdef (mbar): double (nullable = true)\n",
            " |-- sh (g/kg): double (nullable = true)\n",
            " |-- H2OC (mmol/mol): string (nullable = true)\n",
            " |-- rho (g/m**3): double (nullable = true)\n",
            " |-- wv (m/s): double (nullable = true)\n",
            " |-- max. wv (m/s): string (nullable = true)\n",
            " |-- wd (deg): string (nullable = true)\n",
            "\n",
            "----Processed Data-----\n",
            "+-------------------+\n",
            "|           T_Scaled|\n",
            "+-------------------+\n",
            "| 0.2486316138663128|\n",
            "|0.24216287941615522|\n",
            "|0.24050422955714046|\n",
            "|0.24382152927516998|\n",
            "| 0.2444849892187759|\n",
            "|0.24813401890860837|\n",
            "| 0.2552662133023718|\n",
            "| 0.2552662133023718|\n",
            "|  0.250456128711229|\n",
            "|0.24183114944435227|\n",
            "|0.23635760490960356|\n",
            "|0.23436722507878582|\n",
            "|0.23486482003649028|\n",
            "|0.23519655000829323|\n",
            "|0.23801625476861832|\n",
            "|0.23801625476861832|\n",
            "|0.23735279482501243|\n",
            "|0.23552827998009618|\n",
            "|0.23503068502239174|\n",
            "|0.23337203516337698|\n",
            "+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "----***----RNN training with time_sequence : 1 and epochs : 25 ----***----\n",
            "\n",
            "\n",
            "----Training Evaluation----\n",
            "MSE: 1.780186925194538e-05\n",
            "RMSE: 0.0042192261437312625\n",
            "MAE: 0.002828853474546907\n",
            "\n",
            "\n",
            "----Testing Evaluation----\n",
            "MSE: 0.06017747527584033\n",
            "RMSE: 0.24531097667214227\n",
            "MAE: 0.2429512215171818\n",
            "\n",
            "\n",
            "----***----RNN training with time_sequence : 1 and epochs : 50 ----***----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual output vs predicted output plot\n",
        "plt.title('RNN')\n",
        "plt.scatter(actual_test,prd_tst)\n",
        "plt.xlabel('Actual Output')\n",
        "plt.ylabel('Predicted Output')\n",
        "plt.legend(['Weather Data'])"
      ],
      "metadata": {
        "id": "vAgspFfMdDJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance metric (RMSE,MSE,MAE) vs time sequence plot\n",
        "time=[1,5,10]\n",
        "#for epochs=25\n",
        "mse=[0.006698249149405647,0.006786473715583001,0.0023744362953510172]\n",
        "rmse=[0.0818428319977116,0.08238005654029985,0.0487281878931591]\n",
        "mae=[0.07379011546905719,0.07313467459501904,0.041061530588503654]\n",
        "#for epochs 50\n",
        "# mse=[0.17913360633229894,0.03965016704468165,]\n",
        "# rmse=[0.42324178235649057,0.19912349696779044,]\n",
        "# mae=[0.422092780183203,0.19474719884296077,]\n",
        "plt.title('RNN')\n",
        "plt.plot(time,rmse,label='RMSE',color='b')\n",
        "plt.plot(time,mse,label='MSE',color='g')\n",
        "plt.plot(time,mae,label='MAE',color='r')\n",
        "plt.xlabel('time sequence')\n",
        "plt.ylabel('performance metric')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "FaYDRvmzbT0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boxeHeBXeQEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}